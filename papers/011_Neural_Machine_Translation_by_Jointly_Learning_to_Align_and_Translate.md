# Neural Machine Translation by Jointly Learning to Align and Translate [[pdf]](https://arxiv.org/pdf/1409.0473)

*Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. "Neural machine translation by jointly learning to align and translate." ICLR 2015*

Conventional machine translation systems consist of many different parts that are developed individually.
Neural machine translation aims to provide a single system that can be optimized end-to-end.
The previously proposed such solutions use an encoder to convert the input into a fixed-length vector that captures all the information.
A decoder then translates the intermediate representation into the output language.
This fixed-length representation is a performance bottleneck since it can only store a certain amount of information, leading to especially bad accuracy for long input sequences.
The present paper proposes attention mechanisms as a solution.
An alignment model learns which parts of the input sequence to pay attention to at what time and is jointly optimized with the translation model itself.

## 1. Introduction

* *Neural machine translation* attempts to train a single network to perform the entire translation
* This is in contrast to traditional *phrase-based translation systems* which consist of many individually tuned systems
* Neural systems are based on *encoder-decoder* architectures. An encoder maps the input to a fixed-length vector and a decoder converts it into the output sequence. Both parts are jointly trained
* The fixed length of the vector makes it difficult to deal with long sequences. This is confirmed by empirical evaluations of existing systems
* This paper presents an encoder-decoder architecture that jointly learns to translate and align (i.e. learns which part of the input to pay attention to)
* Instead of using a single vector that has to capture everything, a sequence of vectors is used and a model decides which elements are currently imported
* This makes it much easier for the model to represent the input sequence, since it does not have to squash everything into a fixed-size vector anymore
* The resulting model significantly improves on previous neural architectures and works nearly as well as phrase-based systems. When not having to deal with unknown words, it has practically the same performance

## 2. Background: Neural Machine Translation

* Translation is equivalent to finding the target sentence *y* that has the highest probability given a source sentence *x*
* In neural machine translation, we approximate this conditional probability using a neural network
* Translation is then done by searching this distribution for the sentence with the largest probability
* Even though neural machine translation was really only proposed in 2013 and 2014, there have already been some great results
* Adding neural components to traditional machine translation systems (e.g. to re-rank candidate translations) has boosted their performance

### 2.1 RNN Encoder-Decoder

* The encoder creates a hidden state for each element of the input sequence. Usually only the last one of these is used
* The decoder takes the context generated by the encoder and the previous output word, and assigns a probability to each new word
* This probability is decomposed using ordered conditionals: *p(a,b,c) = p(a) \* p(b | a) \* p(c | a, b)*

## 3. Learning to Align and Translate

* New architecture is proposed
* Encoder: Bidirectional RNN
* Decoder: Searches the source sentence to find a translation

### 3.1 Decoder: General Description

* Instead of using one context vector for the entire decoding process, we use a new one for each output element
* This context vector is a weighted sum of the hidden states of the encoder and corresponding annotations
* Each pair of (input i, output j)-indices has an annotation that quantifies how relevant the respective input is to the current output
* This annotation is produced by an alignment model, a feedforward network that is jointly trained with everything else
* The attention mechanism makes it possible to spread out information across hidden states, instead of forcing the encoder to try to put all of it into one vector

### 3.2 Encoder: Bidirectional RNN for Annotating Sequences

* A bidirectional RNN is used
* Annotations should capture information about previous and future words

## 4. Experiment Settings

* English-to-French translation
* Is compared to previous encoder-decoder architecture

### 4.1 Dataset

* A lot of data from the European parliament
* 30,000 most frequent words are used. Others are encoded as *[UNK]*

### 4.2 Models

* Models are trained on sentences of up to a length of 30 or 50
* Beam search is used to generate translations. This was first proposed by Sutskever

## 5. Results

### 5.1 Quantitative Results

* Previous encoder-decoder architecture is always outperformed
* Similar to phrase-based systems (*Mose*) if no unknown words are used
* It is evident that the fixed-length vector of the previous architecture is a bottleneck since it handles long sentences much more badly than the new architecture

### 5.2 Qualitative Analysis

* Alignment can be visualized
* Attention is super useful in some situations, e.g. [the] has multiple translations in French, depending on what the next word is. Here, it is crucial to be able to focus on future parts of the sentence
* Some interesting examples for translations of long texts

## 6. Related Work

### 6.1 Learning to Align

* Graves used a similar attention-mechanism for generating handwriting
* His approach was constrained to attention on previous words though. That does not work for machine translation (see the [the] example above)
* The method in the present paper might be too expensive for very long sequences, since a lot of annotations have to be computed

### 6.2 Neural Networks for Machine Translation

* In the past, neural networks were only used as individual components of phrase-based translation systems
* The authors are more interested in the more ambitious task of only using neural networks to translate

## 7. Conclusion

* Conventional sequence models encode the input into a fixed-length vector. This is a problem for translating long sentences
* Instead, we can learn what parts of the input / which hidden vectors of the encoder to pay attention to
* This significantly improves performance on long sentences
* All pieces are jointly trained
* The results are impressive considering that the first proposal for using neural networks to translate was only a year old
* Future work on handling unknown words is left

## Appendix

* The authors use GRUs as the units of the recurrent networks
* The alignment model needs to be evaluated *n \* m* times for an input sequence of length *n* and an output sequence of length *m*. The model should be designed to accommodate that. They use a single-layered network
* Embedding matrices are shared between the forward and backward networks
* The hidden layers all have 1,000 neurons, including the hidden layers of the alignment model. The embedding size is 620
